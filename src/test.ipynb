{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from importlib import reload\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Video\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.io import read_video, read_video_timestamps\n",
    "\n",
    "import kornia as K\n",
    "import kornia.feature as KF\n",
    "from kornia_moons.feature import *\n",
    "from kornia.contrib import ImageStitcher\n",
    "from kornia.geometry.transform import warp_perspective, get_perspective_transform\n",
    "\n",
    "import utils\n",
    "def load_torch_image(fname):\n",
    "    img = K.image_to_tensor(cv2.imread(fname), False).float() /255.\n",
    "    img = K.color.bgr_to_rgb(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"../deep-stabilization/dvs/video/s_114_outdoor_running_trail_daytime/ControlCam_20200930_104820.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'video_fps': 30.020507836629136, 'audio_fps': 48000}\n",
      "video size:  torch.Size([486, 1080, 1920, 3])\n",
      "audio size:  torch.Size([2, 775168])\n"
     ]
    }
   ],
   "source": [
    "video_frames, audio_frames, meta = read_video(fname, end_pts=100, pts_unit=\"sec\")\n",
    "print(meta)\n",
    "print(\"video size: \", video_frames.shape)\n",
    "print(\"audio size: \", audio_frames.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.show_frames(video_frames[:100:10], 2, 5, (30,16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = video_frames[0:1].permute(0,3,1,2).float() / 255\n",
    "img2 = video_frames[100:101].permute(0,3,1,2).float() / 255\n",
    "\n",
    "print(img1.shape)\n",
    "\n",
    "feature1 = transforms.CenterCrop((270*3,480*3))(img1)\n",
    "feature2 = transforms.CenterCrop((270*3,480*3))(img2)\n",
    "\n",
    "feature1 = torch.cat(transforms.FiveCrop(256)(feature1))\n",
    "feature2 = torch.cat(transforms.FiveCrop(256)(feature2))\n",
    "\n",
    "print(feature1.shape)\n",
    "\n",
    "# K.color.rgb_to_grayscale(img1).shape\n",
    "utils.show_frame(feature1[3].permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher2 = KF.LocalFeatureMatcher(\n",
    "    KF.SIFTFeature(2000, device=\"cuda\"),\n",
    "    KF.DescriptorMatcher('smnn', 0.9)\n",
    "    )\n",
    "\n",
    "input_dict = {\"image0\": K.color.rgb_to_grayscale(feature1).cuda(), # LofTR works on grayscale images only \n",
    "              \"image1\": K.color.rgb_to_grayscale(feature2).cuda()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    correspondences = matcher2(input_dict)\n",
    "    del input_dict[\"image0\"], input_dict[\"image1\"]\n",
    "    \n",
    "for k,v in correspondences.items():\n",
    "    print (k)\n",
    "\n",
    "print(len(correspondences[\"keypoints0\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in range(5):\n",
    "#     idx = torch.topk(correspondences[\"confidence\"][correspondences[\"batch_indexes\"]==x], 100).indices\n",
    "#     print((correspondences[\"keypoints0\"][correspondences[\"batch_indexes\"]==x][idx] - correspondences[\"keypoints1\"][correspondences[\"batch_indexes\"]==x][idx]).mean(dim=0))\n",
    "# print(\"\\n\\n\\n\")\n",
    "# for x in range(5):\n",
    "#     idx = torch.topk(correspondences[\"confidence\"][correspondences[\"batch_indexes\"]==x], 150).indices\n",
    "#     print((correspondences[\"keypoints0\"][correspondences[\"batch_indexes\"]==x][idx] - correspondences[\"keypoints1\"][correspondences[\"batch_indexes\"]==x][idx]).mean(dim=0))\n",
    "# print(\"\\n\\n\\n\")\n",
    "tmp = []\n",
    "for x in range(5):\n",
    "    tmp.append((correspondences[\"keypoints0\"][correspondences[\"batch_indexes\"]==x] - correspondences[\"keypoints1\"][correspondences[\"batch_indexes\"]==x]).median(dim=0)[0])\n",
    "    print(tmp[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = torch.Tensor([\n",
    "    [135*1+128, 240*1+128],# 左上\n",
    "    [135*1+128, 240*7-128],# 右上\n",
    "    [135*7-128, 240*1+128],# 左下\n",
    "    [135*7-128, 240*7-128] # 右下\n",
    "]).cuda()\n",
    "\n",
    "dst = torch.vstack(tmp[:4]) + src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1[0].permute(1,2,0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = cv2.warpAffine(img1[0].permute(1,2,0).numpy(), H[:2], (1080, 1920))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.show_frame(torch.from_numpy(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H, inliers = cv2.findFundamentalMat(mkpts0, mkpts1, cv2.USAC_MAGSAC, 0.5, 0.999, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(src)\n",
    "print(dst)\n",
    "b = get_perspective_transform(src.unsqueeze(0), dst.unsqueeze(0))\n",
    "\n",
    "out = warp_perspective(img1.cuda(), b, (1080,1920)).cpu()\n",
    "outt = torch.where(out == 0.0, img2, out)\n",
    "utils.show_frame(outt[0].permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = warp_perspective(img1.cuda(), torch.from_numpy(H).cuda().unsqueeze(0).float(), (1080,1920)).cpu()\n",
    "outtt = torch.where(out == 0.0, img2, out)\n",
    "utils.show_frame(outtt[0].permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th = torch.quantile(correspondences[\"confidence\"], 0.0)\n",
    "idx = correspondences[\"confidence\"] > th\n",
    "print(idx.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkpts0 = correspondences['keypoints0'][idx].cpu().numpy()\n",
    "mkpts1 = correspondences['keypoints1'][idx].cpu().numpy()\n",
    "H, inliers = cv2.findFundamentalMat(mkpts0, mkpts1, cv2.USAC_MAGSAC, 0.5, 0.999, 100000)\n",
    "inliers = inliers > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_LAF_matches(\n",
    "    KF.laf_from_center_scale_ori(torch.from_numpy(mkpts0).view(1,-1, 2),\n",
    "                                torch.ones(mkpts0.shape[0]).view(1,-1, 1, 1),\n",
    "                                torch.ones(mkpts0.shape[0]).view(1,-1, 1)),\n",
    "\n",
    "    KF.laf_from_center_scale_ori(torch.from_numpy(mkpts1).view(1,-1, 2),\n",
    "                                torch.ones(mkpts1.shape[0]).view(1,-1, 1, 1),\n",
    "                                torch.ones(mkpts1.shape[0]).view(1,-1, 1)),\n",
    "    torch.arange(mkpts0.shape[0]).view(-1,1).repeat(1,2),\n",
    "    K.tensor_to_image(img1),\n",
    "    K.tensor_to_image(img2),\n",
    "    inliers,\n",
    "    draw_dict={'inlier_color': (0.2, 1, 0.2),\n",
    "               'tentative_color': None, \n",
    "               'feature_color': (0.2, 0.5, 1), 'vertical': False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kornia.geometry.transform import get_perspective_transform, warp_perspective\n",
    "idx = torch.topk(correspondences[\"confidence\"], 12).indices\n",
    "# idx = torch.randperm(20)\n",
    "src = correspondences[\"keypoints0\"][idx[:4]].unsqueeze(0)\n",
    "dst = correspondences[\"keypoints1\"][idx[:4]].unsqueeze(0)\n",
    "a = get_perspective_transform(src, dst)\n",
    "src = correspondences[\"keypoints0\"][idx[2:6]].unsqueeze(0)\n",
    "dst = correspondences[\"keypoints1\"][idx[2:6]].unsqueeze(0)\n",
    "b = get_perspective_transform(src, dst)\n",
    "\n",
    "out = warp_perspective(img1.cuda(), (a+b)/2, (1080//4,1920//4)).cpu()\n",
    "outt = torch.where(out < 0.0, img2, out)\n",
    "utils.show_frame(outt[0].permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1920 1080\n",
      "transforms:  485  Tracked points : 128\n"
     ]
    }
   ],
   "source": [
    "# Import numpy and OpenCV\n",
    "import numpy as np\n",
    "import cv2# Read input video\n",
    "\n",
    "fname = \"../deep-stabilization/dvs/video/s_114_outdoor_running_trail_daytime/ControlCam_20200930_104820.mp4\"\n",
    "cap = cv2.VideoCapture(fname)\n",
    " \n",
    "# Get frame count\n",
    "n_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    " \n",
    "# Get width and height of video stream\n",
    "w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    " \n",
    "# Define the codec for output video\n",
    " \n",
    "# Set up output video\n",
    "fps = 30\n",
    "print(w, h)\n",
    "\n",
    "# Read first frame\n",
    "_, prev = cap.read()\n",
    " \n",
    "# Convert frame to grayscale\n",
    "prev_gray = cv2.cvtColor(prev, cv2.COLOR_BGR2GRAY)\n",
    "# prev_gray = (prev_gray&192)|((prev_gray&32)<<1)\n",
    "\n",
    "# Pre-define transformation-store array\n",
    "transforms = np.zeros((n_frames-1, 3), np.float32)\n",
    "log = []\n",
    "homo = []\n",
    "for i in range(n_frames-2):\n",
    "    log.append([])\n",
    "    # Detect feature points in previous frame\n",
    "    prev_pts = cv2.goodFeaturesToTrack(prev_gray,\n",
    "                                     maxCorners=400,\n",
    "                                     qualityLevel=0.3,\n",
    "                                     minDistance=20,\n",
    "                                     blockSize=9)\n",
    "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n",
    "    prev_pts = cv2.cornerSubPix( prev_gray, prev_pts, (5,5), (-1,1), criteria )\n",
    "    # Read next frame\n",
    "    success, curr = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    " \n",
    "    # Convert to grayscale\n",
    "    curr_gray = cv2.cvtColor(curr, cv2.COLOR_BGR2GRAY)\n",
    " \n",
    "    # Calculate optical flow (i.e. track feature points)\n",
    "    curr_pts, status, err = cv2.calcOpticalFlowPyrLK(prev_gray, curr_gray, prev_pts, None)\n",
    " \n",
    "    # Sanity check\n",
    "    assert prev_pts.shape == curr_pts.shape\n",
    " \n",
    "    # Filter only valid points\n",
    "    idx = np.where(status==1)[0]\n",
    "    prev_pts = prev_pts[idx]\n",
    "    curr_pts = curr_pts[idx]\n",
    " \n",
    "    #Find transformation matrix\n",
    "    retval, inliers = cv2.estimateAffine2D(prev_pts, curr_pts)\n",
    "    retval = cv2.findHomography(prev_pts, curr_pts)[0]\n",
    "    homo.append(cv2.findHomography(prev_pts, curr_pts)[0])\n",
    "    # \"\"\"\n",
    "    # adding\n",
    "    # \"\"\"\n",
    "    # cv2.find(prev_pts, curr_pts)\n",
    "    # arr = np.arange(prev_pts.shape[0])\n",
    "    # tests = []\n",
    "    # for x in range(100):\n",
    "    #     index = np.random.choice(prev_pts.shape[0], size=(4,), replace=False)\n",
    "    #     tests.append(cv2.getPerspectiveTransform(prev_pts[index], curr_pts[index]))\n",
    "    # test = np.stack(tests)\n",
    " \n",
    "    # Extract traslation\n",
    "    dx = retval[0][2]\n",
    "    dy = retval[1][2]\n",
    " \n",
    "    # Extract rotation angle\n",
    "    da = np.arctan2(retval[1,0], retval[0,0])\n",
    "    log[-1].append(len(inliers))\n",
    "    log[-1].append(np.arctan2(retval[0,1], retval[1,1]))\n",
    " \n",
    "    # Store transformation\n",
    "    transforms[i] = [dx,dy,da]\n",
    " \n",
    "    # Move to next frame\n",
    "    prev_gray = curr_gray\n",
    " \n",
    "    print(\"Frame: {:03d}/{:3d} -  Tracked points : {:3d}\".format(i, n_frames, len(prev_pts)), end=\"\\r\", flush=True)\n",
    "  \n",
    "# Compute trajectory using cumulative sum of transformations\n",
    "print(\"transforms: \", len(transforms))\n",
    "trajectory = np.cumsum(transforms, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "intrinsic = np.array([\n",
    "        [1920/1.27, 0.0, 0.5*(1920-1)], \n",
    "        [0.0, 1920/1.27, 0.5*(1080-1)], \n",
    "        [0.0, 0.0, 1.0]\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = []\n",
    "arr = np.arange(prev_pts.shape[0])\n",
    "for x in range(100):\n",
    "    index = np.random.choice(prev_pts.shape[0], size=(10,), replace=False)\n",
    "    tests.append(cv2.findFundamentalMat(prev_pts[index], curr_pts[index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -23.1438,   13.7808,  -45.6928,   -0.8982],\n",
       "        [  13.1808, -117.8158,   -5.3662,   -0.1274],\n",
       "        [ -45.4236,   -6.2476,  -99.1835,    0.4207]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import kornia\n",
    "p_pts = torch.from_numpy(prev_pts).permute(1,0,2)\n",
    "c_pts = torch.from_numpy(curr_pts).permute(1,0,2)\n",
    "pts, tran = kornia.geometry.epipolar.normalize_points(torch.cat([p_pts, c_pts], dim=1))\n",
    "p_pts, c_pts = pts.narrow(1,0,128), pts.narrow(1,128,128)\n",
    "fund1 = kornia.geometry.epipolar.find_fundamental(p_pts, c_pts, weights=torch.ones((1,128)))\n",
    "kornia.geometry.epipolar.projections_from_fundamental(fund1)[0, :, : ,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-7.2350e-03,  2.8201e-02, -2.5415e-01, -9.6713e-01],\n",
       "        [ 2.7516e-02, -1.0726e-01,  9.6716e-01, -2.5428e-01],\n",
       "        [ 1.3312e-04,  3.4640e-05, -1.1435e-01, -1.1996e-03]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fund2 = cv2.findFundamentalMat(prev_pts, curr_pts)[0]\n",
    "fund2 = torch.from_numpy(fund2).view(1,3,3)\n",
    "kornia.geometry.epipolar.projections_from_fundamental(fund2)[0, :, : ,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter\n",
    "def movingAverage(curve, window_size):\n",
    "    # Define the filter\n",
    "    f = np.ones(window_size)/window_size\n",
    "    # Add padding to the boundaries\n",
    "    curve_pad = np.lib.pad(curve, (window_size-3, 2), 'edge')\n",
    "    # Apply convolution\n",
    "    curve_smoothed = np.convolve(curve_pad, f, mode='valid')\n",
    "    # Remove padding\n",
    "    curve_smoothed = curve_smoothed\n",
    "    # return smoothed curve\n",
    "    return savgol_filter(curve, window_size, 3)\n",
    "    # return curve_smoothed\n",
    "# def movingAverage(curve, radius):\n",
    "#     window_size = 2 * radius + 1\n",
    "#     # Define the filter\n",
    "#     f = np.ones(window_size)/window_size\n",
    "#     # Add padding to the boundaries\n",
    "#     curve_pad = np.lib.pad(curve, (radius, radius), 'edge')\n",
    "#     # Apply convolution\n",
    "#     curve_smoothed = np.convolve(curve_pad, f, mode='same')\n",
    "#     # Remove padding\n",
    "#     curve_smoothed = curve_smoothed[radius:-radius]\n",
    "#     # return smoothed curve\n",
    "#     return savgol_filter(curve, window_size, 3)\n",
    "#     # return curve_smoothed\n",
    "\n",
    "def fixBorder(frame):\n",
    "    s = frame.shape\n",
    "    # Scale the image 4% without moving the center\n",
    "    T = cv2.getRotationMatrix2D((s[1]/2, s[0]/2), 0, 1.04)\n",
    "    frame = cv2.warpAffine(frame, T, (s[1], s[0]))\n",
    "    return frame\n",
    "\n",
    "def smooth(trajectory, SMOOTHING_RADIUS=31):\n",
    "    smoothed_trajectory = np.copy(trajectory)\n",
    "    # Filter the x, y and angle curves\n",
    "    for i in range(3):\n",
    "        smoothed_trajectory[:,i] = movingAverage(trajectory[:,i], SMOOTHING_RADIUS)\n",
    " \n",
    "    return smoothed_trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps, w, h = 30, 1920, 1080\n",
    "# Calculate difference in smoothed_trajectory and trajectory\n",
    "smoothed_trajectory = smooth(trajectory)\n",
    "difference = smoothed_trajectory - trajectory\n",
    "transforms_smooth = transforms + difference\n",
    "\n",
    "\n",
    "# Reset stream to first frame\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "\n",
    "frames=[]\n",
    "# Write n_frames-1 transformed frames\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter('../video_out.mp4', fourcc, fps, (w, h))\n",
    "for i in range(n_frames-2):\n",
    "    # Read next frame\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # Extract transformations from the new transformation array\n",
    "    dx = transforms_smooth[i,0]\n",
    "    dy = transforms_smooth[i,1]\n",
    "    da = transforms_smooth[i,2]\n",
    " \n",
    "    # Reconstruct transformation matrix accordingly to new values\n",
    "    m = np.zeros((3,3), np.float32)\n",
    "    m[0,0] = np.cos(da)\n",
    "    m[0,1] = -np.sin(da)\n",
    "    m[1,0] = np.sin(da)\n",
    "    m[1,1] = np.cos(da)\n",
    "    m[0,2] = dx\n",
    "    m[1,2] = dy\n",
    "    m[2] = homo[i][2]\n",
    " \n",
    "    # Apply affine wrapping to the given frame\n",
    "    # frame_stabilized = cv2.warpAffine(frame.astype(np.float64)/255, m, (w,h))\n",
    "    # tmp = sqrtm(sqrtm(sqrtm(sqrtm(acc_homo[i]@np.linalg.inv(acc_homo[max(0,i-16)]))))).real\n",
    "    # tmp = homo[i]@tmp@np.linalg.inv(acc_homo[i])\n",
    "    # tmp[2] = homo[i][2]\n",
    "    frame_stabilized = cv2.warpPerspective(frame.astype(np.float64)/255, m, (w,h))\n",
    "\n",
    "    # Fix border artifacts\n",
    "    # frame_stabilized = fixBorder(frame_stabilized)\n",
    "\n",
    "    # Write the frame to the file\n",
    "    frame_out = cv2.hconcat([frame.astype(np.float64)/255, frame_stabilized])\n",
    "\n",
    "    # If the image is too big, resize it.\n",
    "    if frame_out.shape[1] > 1920:\n",
    "        frame_out = cv2.resize(frame_out, (frame_out.shape[1]//2, frame_out.shape[0]));\n",
    " \n",
    "    frames.append(frame_out)\n",
    "    out.write((frame_out*255).astype(np.uint8))\n",
    "\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.linalg as LA\n",
    "from torch.autograd import Variable\n",
    "def norm_quat(quat):\n",
    "    norm_quat = LA.norm(quat)   \n",
    "    if norm_quat > 1e-6:\n",
    "        quat = quat / norm_quat   \n",
    "        #     [0 norm_quat norm_quat - 1e-6]\n",
    "    else:\n",
    "        # print('bad len for Reciprocal')\n",
    "        quat = np.array([0,0,0,1])\n",
    "    return quat\n",
    "\n",
    "def torch_norm_quat(quat, USE_CUDA = True):\n",
    "    # Method 1:\n",
    "    batch_size = quat.size()[0]\n",
    "    quat_out = Variable(torch.zeros((batch_size, 4), requires_grad=True))\n",
    "    if USE_CUDA == True:\n",
    "        quat_out = quat_out.cuda()\n",
    "    for i in range(batch_size):\n",
    "        norm_quat = torch.norm(quat[i])   \n",
    "        if norm_quat > 1e-6:        \n",
    "            quat_out[i] = quat[i] / norm_quat  \n",
    "            #     [0 norm_quat norm_quat - 1e-6]\n",
    "        else:\n",
    "            quat_out[i,:3] = quat[i,:3] * 0\n",
    "            quat_out[i,3] = quat[i,3] / quat[i,3]\n",
    "\n",
    "    # Method 2:\n",
    "    # quat = quat / (torch.unsqueeze(torch.norm(quat, dim = 1), 1) + 1e-6) # check norm\n",
    "    return quat_out\n",
    "\n",
    "def QuaternionReciprocal(q):\n",
    "    quat = np.array([-q[0], -q[1], -q[2], q[3]])  \n",
    "    return norm_quat(quat)\n",
    "\n",
    "def torch_QuaternionReciprocal(q,  USE_CUDA = True):\n",
    "    quat = torch.cat((-q[:,0:1], -q[:,1:2], -q[:,2:3], q[:,3:]), dim = 1) \n",
    "    batch_size = quat.size()[0]\n",
    "\n",
    "    quat = torch_norm_quat(quat)\n",
    "    return quat\n",
    "\n",
    "def QuaternionProduct(q1, q2):\n",
    "    x1 = q1[0]  \n",
    "    y1 = q1[1]   \n",
    "    z1 = q1[2]   \n",
    "    w1 = q1[3]   \n",
    "\n",
    "    x2 = q2[0]  \n",
    "    y2 = q2[1]  \n",
    "    z2 = q2[2]  \n",
    "    w2 = q2[3]  \n",
    "\n",
    "    quat = np.zeros(4)\n",
    "    quat[3] =  w1 * w2 - x1 * x2 - y1 * y2 - z1 * z2  \n",
    "    quat[0] =  w1 * x2 + x1 * w2 + y1 * z2 - z1 * y2  \n",
    "    quat[1] = w1 * y2 - x1 * z2 + y1 * w2 + z1 * x2  \n",
    "    quat[2] = w1 * z2 + x1 * y2 - y1 * x2 + z1 * w2 \n",
    "\n",
    "    return norm_quat(quat)\n",
    "\n",
    "def torch_QuaternionProduct(q1, q2, USE_CUDA = True):\n",
    "    x1 = q1[:,0]  \n",
    "    y1 = q1[:,1]   \n",
    "    z1 = q1[:,2]   \n",
    "    w1 = q1[:,3]   \n",
    "\n",
    "    x2 = q2[:,0]  \n",
    "    y2 = q2[:,1]  \n",
    "    z2 = q2[:,2]  \n",
    "    w2 = q2[:,3]  \n",
    "\n",
    "    batch_size = q1.size()[0]\n",
    "    quat = Variable(torch.zeros((batch_size, 4), requires_grad=True))\n",
    "    if USE_CUDA == True:\n",
    "        quat = quat.cuda()\n",
    "    \n",
    "    quat[:,3] =  w1 * w2 - x1 * x2 - y1 * y2 - z1 * z2  \n",
    "    quat[:,0] =  w1 * x2 + x1 * w2 + y1 * z2 - z1 * y2  \n",
    "    quat[:,1] = w1 * y2 - x1 * z2 + y1 * w2 + z1 * x2  \n",
    "    quat[:,2] = w1 * z2 + x1 * y2 - y1 * x2 + z1 * w2  \n",
    "\n",
    "    quat = torch_norm_quat(quat)\n",
    "\n",
    "    return quat\n",
    "\n",
    "def get_data_at_timestamp(self, gyro_data, ois_data, time_stamp, quat_t_1):\n",
    "    quat_t = GetGyroAtTimeStamp(gyro_data, time_stamp)\n",
    "    quat_dif = QuaternionProduct(quat_t, QuaternionReciprocal(quat_t_1))  \n",
    "    return quat_dif\n",
    "\n",
    "def get_relative_quats(quat):\n",
    "    num_inputs = quat.shape[0]\n",
    "    quats = np.zeros((num_inputs, 4))  \n",
    "    quats[0,:] = np.array([0, 0, 0, 1])\n",
    "    for i in range(1, num_inputs):\n",
    "        quats[i,:] = QuaternionProduct(quat[i], quats[i-1,:])   # R_t = delta R_t * R_t-1\n",
    "        quats[i,:] = quats[i,:] / LA.norm(quats[i,:]) \n",
    "    return quats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([472, 4])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00],\n",
       "       [ 1.07243783e-04, -2.41898522e-04, -2.08230014e-05,\n",
       "         9.99999965e-01],\n",
       "       [ 4.01388428e-04, -3.33386593e-04,  5.98872714e-04,\n",
       "         9.99999685e-01],\n",
       "       ...,\n",
       "       [-6.96774177e-01, -4.17902098e-01, -4.37305643e-01,\n",
       "         3.85522187e-01],\n",
       "       [-6.96078259e-01, -4.17020978e-01, -4.38120507e-01,\n",
       "         3.86806132e-01],\n",
       "       [-6.95171253e-01, -4.16523131e-01, -4.39439293e-01,\n",
       "         3.87477119e-01]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from kornia.geometry.conversions import rotation_matrix_to_quaternion, QuaternionCoeffOrder, normalize_homography\n",
    "n_homo = normalize_homography(torch.from_numpy(np.array(homo)), (1080,1920), (1080,1920))\n",
    "quats = rotation_matrix_to_quaternion(n_homo, order=QuaternionCoeffOrder.WXYZ)[10:-2,[1,2,3,0]]\n",
    "get_relative_quats(quats.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.33650631e+00  5.72309689e-02 -3.71291010e+02]\n",
      " [ 3.95840586e-02  1.16016210e+00 -7.96622217e+01]\n",
      " [ 1.20057797e-04  1.91982145e-05  9.03927825e-01]]\n",
      "[[ 4.65115394e+02  2.79564289e+02 -6.86257487e+05]\n",
      " [ 7.55197021e+01  1.39863309e+02 -1.38874281e+05]\n",
      " [ 2.03738676e-01  1.18128286e-01 -2.95844101e+02]]\n",
      "[[  0.45065314  -0.8926991  316.80075   ]\n",
      " [  0.8926991    0.45065314 177.64972   ]]\n"
     ]
    }
   ],
   "source": [
    "x = sqrtm(sqrtm(sqrtm(sqrtm(sqrtm(acc_homo[i]))))).real\n",
    "print(x)\n",
    "print(acc_homo[i])\n",
    "dx = transforms_smooth[i,0]\n",
    "dy = transforms_smooth[i,1]\n",
    "da = transforms_smooth[i,2]\n",
    "\n",
    "# Reconstruct transformation matrix accordingly to new values\n",
    "m = np.zeros((2,3), np.float32)\n",
    "m[0,0] = np.cos(da)\n",
    "m[0,1] = -np.sin(da)\n",
    "m[1,0] = np.sin(da)\n",
    "m[1,1] = np.cos(da)\n",
    "m[0,2] = dx\n",
    "m[1,2] = dy\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import read_video, read_video_timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"../video_out.mp4\" controls  width=\"960\"  height=\"540\">\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "Video(\"../video_out.mp4\", width=960, height=540)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(\"../stable_video.avi\", width=960, height=540)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b14ac9e988f69289b67bb64ec11e6e1cb3880eab5602089907734852574a6011"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('ml': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
